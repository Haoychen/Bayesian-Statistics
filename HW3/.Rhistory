census.data <- read.csv("census.csv", header = T)
head(census.data)
census.sample <- census.data[is.na(census.data) == FALSE]
head(census.sample)
census.sample <- census.data[is.na(census.data$outcome) == FALSE,]
head(census.sample)
census.prediction <- census.data[is.na(census.data$outcome) == TRUE,]
head(census.prediction)
census.data <- read.csv("census.csv", header = T)
with(census.data, {
educ <<- factor(educ)
status <<- factor(status)
race <<- factor(race)
gender <<- factor(gender)
})
census.sample <- census.data[is.na(census.data$outcome) == FALSE,]
census.prediction <- census.data[is.na(census.data$outcome) == TRUE,]
head(census.sample)
character(census.data$educ)
character(census.data$age)
help(character)
is.factor(census.data$educ)
logistic.fit = glm(outcome ~ age + educ + status + race + gender + hrs, data = census.sample, family = binomial)
summary(logistic.fit)
logistic.fit <- glm(outcome ~ age + educ + status + race + gender + hrs, data = census.sample, family = binomial)
summary(logistic.fit)
library(ISLR)
set.seed(1)
library(boot)
cv.err <- cv.glm(census.sample, logistic.fit)
dim(census.sample)
library(MASS)
help(qda)
qda.fit <- qda(outcome ~ age + educ + status + race + gender + hrs, data = census.sample)
summary(qda.fit)
predict(qda, newdata = data.frame(age = 10, educ = 'Bachelor', Status = 'Divorced', race = 'white', gender = 'Female'))
head(census.sample)
predict(qda, newdata = census.sample[1, 1 - 7], gender = 'Female'))
predict(qda, newdata = census.sample[1, c(1-7)], gender = 'Female'))
predict(qda, newdata = census.sample[1, c(1, 2, 3, 4, 5, 6)], gender = 'Female'))
predict(qda, newdata = census.sample[1, c(1, 2, 3, 4, 5, 6)])
predict(qda, newdata = census.sample[1, 1 : 6])
predict(qda, census.sample[1, 1 : 6])
predict(qda, census.sample[, 1:6])
predict(qda.fit, census.sample[, 1:6])
predict(qda.fit, census.sample[1, 1:6])
predict(qda.fit, census.sample[1:2, 1:6])
qda.class <- predict(qda.fit, census.sample[, 1:6])$class
table(qda.class, census.sample[, 7])
mean(qda.calss == census.sample[, 7])
mean(qda.class == census.sample[, 7])
logistic.fit <- predict(logistic.fit, census.sample[, 1:6])$class
predict(logistic.fit, census.sample[, 1:6])
predict(logistic.fit, census.sample[1, 1:6])
predict(logistic.fit, census.sample[1:2, 1:6])
logistic.fit <- glm(outcome ~ age + educ + status + race + gender + hrs, data = census.sample, family = binomial)
predict(logistic.fit, census.sample[1:2, 1:6])
logistic.fit <- glm(outcome ~ age + educ + status + race + gender + hrs, data = census.sample, family = "binomial")
predict(logistic.fit, census.sample[1:2, 1:6])
predict(logistic.fit, census.sample[1:10, 1:6])
predict(logistic.fit, census.sample[1:10, 1:6], type = 'response')
logistic.value <- predict(logistic.fit, census.sample[, 1:6], type = 'response')
mean(logistic.value)
logistic.value[logistic.value >= .5] = 1
logistic.value[logistic.value < .5] = 0
table(logistic.value, census.sample[, 7])
mean(logistic.value == census.sample[, 7])
step(logistic.fit, direction = "both")
step(logistic.fit, direction = "backward")
logistic.value <- predict(logistic.fit, census.prediction[, 1:6])
head(logistic.value)
logistic.value <- predict(logistic.fit, census.prediction[, 1:6], type = "response")
head(logistic.value)
logistic.value[logistic.value > .5] <- 1
logistic.value[logistic.value < .5] <- 0
head(logistic.value)
dim(logistic.value)
census.prediction$outcome <- logistic.value
output.data <- rbind(census.sample[, 7: 8], census.prediction[, 7: 8])
head(output.data)
output.data <- output.data[, c('id', 'outcome')]
head(output.data)
output.data <- output.data[order(output.data$id),]
head(output.data)
View(output.data)
predict(qda.fit, census.prediction[,1: 6])$class
predict(qda.fit, census.prediction[1,1: 6])$class
head(census.prediction)
predict(qda.fit, census.prediction[2,1: 6])$class
predict(qda.fit, census.prediction[3,1: 6])$class
predict(qda.fit, census.prediction[1:6,1: 6])$class
write.table(output.data, "hw5.txt")
write.table(output.data, "hw5.txt", sep = '\t')
View(output.data)
write.table(output.data, "hw5.txt", sep = '\t')
help(write.table)
dim(output.data)
write.table(output.data, "hw5.txt", sep = '\t')
write.table(output.data, "hw5.txt", col.names = c("id", "outcome"), row.names = output.data[,1] sep = '\t')
write.table(output.data, "hw5.txt", col.names = c("id", "outcome"), row.names = output.data[,1], sep = '\t')
write.table(output.data, "hw5.csv")
write.table(output.data, "hw5.csv", sep = '\t')
View(output.data)
write.csv(output.data, "hw5.csv")
input.data <- read.table("hw5.txt", header = T)
head(input.data)
View(census.data)
View(census.prediction)
census.data <- read.csv("census.csv", header = T)
with(census.data, {
educ <<- factor(educ)
status <<- factor(status)
race <<- factor(race)
gender <<- factor(gender)
})
census.sample <- census.data[is.na(census.data$outcome) == FALSE,]
census.prediction <- census.data[is.na(census.data$outcome) == TRUE,]
logistic.fit <- glm(outcome ~ age + educ + status + race + gender + hrs, data = census.sample, family = binomial)
logistic.value <- predict(logistic.fit, census.prediction[, 1:6], type = 'response')
logistic.value[logistic.value > .5] <- 1
logistic.value[logistic.value < .5] <- 0
output.data <- cbind(logistic.value, census.prediction[,8])
View(output.data)
write.table(logistc.value, 'output.txt', sep = '\t')
write.table(logistic.value, 'output.txt', sep = '\t')
write.table(logistic.value, 'output.txt', sep = '\t', colname = c('id', 'outcome'))
write.table(output.data, 'output.txt', sep = '\t')
output <- output[,c(2,1)]
output.data <- output.data[,c(2,1)]
write.table(output.data, 'output.txt', sep = '\t')
dim(output.data)
names(output.data) <- c('id', 'outcome')
write.table(output.data, 'output.txt', sep = '\t')
write.table(output.data, 'output.txt', sep = '\t')
library(FSelector)
if(!require(installr)) {
install.packages("installr"); require(installr)} #load / install+load installr
# using the package:
updateR()
updateR()
??updateR
library(installr)
install.packages('dplyr')
help(dplyr)
install.packages('knitr')
vignette('datatable-intro')
install.packages(plotly)
install.packages('plotly')
??boxcox
??dpyr
help(pt)
setwd("~/Courses/Bayesian-Statistics/HW3")
y <- c(24,25,31,31,22,21,26,20,16,22)alpha <- seq(mean(y) - 4 * sd(y), mean(y) + 4*sd(y), length.out = 1000)beta <- seq(-4, 4, length.out = 1000)prior<-matrix(rep(0,1000000), 1000, 1000)for (i in 1:1000){    for (j in 1:1000){        if (alpha[i] + 12 * beta[j] > 0){            prior[i, j] <- exp(-(alpha[i] - ((mean(y)) ^ 2) * 0.5) / var(y) - beta[j] ^ 2 / 2)        }    }}contour(alpha,beta,prior,xlim=c(10,40),ylim=c(-4,4),xlab="alpha",ylab="beta",main="contour of informative prior distribution")
y <- c(24,25,31,31,22,21,26,20,16,22)
alpha <- seq(mean(y) - 4 * sd(y), mean(y) + 4*sd(y), length.out = 1000)
beta <- seq(-4, 4, length.out = 1000)
for (i in 1:1000){
for (j in 1:1000){
if (alpha[i] + 12 * beta[j] > 0){
prior[i, j] <- exp(-(alpha[i] - ((mean(y)) ^ 2) * 0.5) / var(y) - beta[j] ^ 2 / 2)
}
}
}
contour(alpha,beta,prior,xlim=c(10,40),ylim=c(-4,4),xlab="alpha",ylab="beta",main="contour of informative prior distribution")
prior<-matrix(rep(0,1000000), 1000, 1000)
y <- c(24,25,31,31,22,21,26,20,16,22)
alpha <- seq(mean(y) - 4 * sd(y), mean(y) + 4*sd(y), length.out = 1000)
beta <- seq(-4, 4, length.out = 1000)
prior<-matrix(rep(0,1000000), 1000, 1000)
for (i in 1:1000){
for (j in 1:1000){
if (alpha[i] + 12 * beta[j] > 0){
prior[i, j] <- exp(-(alpha[i] - ((mean(y)) ^ 2) * 0.5) / var(y) - beta[j] ^ 2 / 2)
}
}
}
contour(alpha,beta,prior,xlim=c(10,40),ylim=c(-4,4),xlab="alpha",ylab="beta",main="contour of informative prior distribution")
y <- c(24,25,31,31,22,21,26,20,16,22)alpha <- seq(mean(y) - 4 * sd(y), mean(y) + 4*sd(y), length.out = 1000)beta <- seq(-4, 4, length.out = 1000)prior<-matrix(rep(0,1000000), 1000, 1000)for (i in 1:1000){    for (j in 1:1000){        if (alpha[i] + 12 * beta[j] > 0){            prior[i, j] <- exp(-(alpha[i] - mean(y)) ^ 2 * 0.5 / var(y) - beta[j] ^ 2 / 2)        }    }}contour(alpha,beta,prior,xlim=c(10,40),ylim=c(-4,4),xlab="alpha",ylab="beta",main="contour of informative prior distribution")
y <- c(24,25,31,31,22,21,26,20,16,22)
alpha <- seq(mean(y) - 4 * sd(y), mean(y) + 4*sd(y), length.out = 1000)
beta <- seq(-4, 4, length.out = 1000)
prior<-matrix(rep(0,1000000), 1000, 1000)
for (i in 1:1000){
for (j in 1:1000){
if (alpha[i] + 12 * beta[j] > 0){
prior[i, j] <- exp(-(alpha[i] - mean(y)) ^ 2 * 0.5 / var(y) - beta[j] ^ 2 / 2)
}
}
}
contour(alpha,beta,prior,xlim=c(10,40),ylim=c(-4,4),xlab="alpha",ylab="beta",main="contour of informative prior distribution")
t <- seq(1:10)fit <- lm(y ~ t)summary(fit)
t <- seq(1:10)
fit <- lm(y ~ t)
summary(fit)
help(prod)
posterior <- matrix(rep(0, 1000000), 1000, 1000)
for (i in 1:1000){
for (j in 1:1000){
if (alpha[i] + 12 * beta[j] > 0){
# simulate posterior
posterior[i, j] <- prod(exp(-(alpha[i] + beta[j] * t)) * (alpha[i] + beta[j] * t) ^ y)
}
}
}
contour(alpha, beta, posterior, xlim=c(10,40), ylim=c(-4,4), xlab="alpha", ylab="beta", main="contour of joint posterior density")
#take 1000 draws from the joint posterior density
post_alpha <- apply(posterior, 1, sum)
alpha_sim<-rep(0, 1000)
beta_sim<-rep(0, 1000)
for (k in 1:1000){
i <- sample(1000, 1, prob = post_alpha)
j <- sample(1000, 1, prob=posterior[i,])
alpha_sim[k] <- alpha[i]
beta_sim[k] <- beta[j]
}
plot(alpha_sim, beta_sim, xlim=c(10,40), ylim=c(-4,4), xlab="alpha", ylab="beta", main="simulation from the joint posterior density")
hist(alpha_sim + 11 * beta_sim, breaks = 20, xlab="expected number", main="posterior density for the expected number of fatal accidents")
help(rpois)
sim<-rpois(1000, alpha_sim + 11 * beta_sim)
quantile(sim, c(0.025,0.975))
contour(alpha, beta, prior, xlim=c(10,40), ylim=c(-4,4),xlab="alpha",ylab="beta")
points(alpha_sim, beta_sim)
theta <- seq(-10, 10, 0.01)
p <- pnorm(theta/4 + 0.675 * sqrt(5/4)) - pnorm(theta / 4 - 0.675 * sqrt(5/4))
plot(theta, p, type = 'l')
p_theta <- -1 / sqrt(8 * 3.14159265) * exp(-theta ^ 2 /8)
sum(0.01 *p * p_theta)
theta <- seq(-10, 10, 0.01)
p <- - pnorm(theta/4 + 0.675 * sqrt(5/4)) - pnorm(theta / 4 - 0.675 * sqrt(5/4))
p_theta <- -1 / sqrt(8 * 3.14159265) * exp(-theta ^ 2 /8)
sum(0.01 *p * p_theta)
# c
plot(theta, p, type = 'l')
theta <- -seq(-10, 10, 0.01)
p <- -pnorm(theta/4 + 0.675 * sqrt(5/4)) - pnorm(theta / 4 - 0.675 * sqrt(5/4))
p_theta <- -1 / sqrt(8 * 3.14159265) * exp(-theta ^ 2 /8)
sum(0.01 *p * p_theta)
# c
plot(theta, p, type = 'l')
theta <- -seq(-10, 10, 0.01)
theta <- -seq(-10, 10, 0.01)
theta <- seq(10, -10, 0.01)
theta <- -seq(-10, 10, 0.01)
p <- -pnorm(theta/4 + 0.675 * sqrt(5/4)) - pnorm(theta / 4 - 0.675 * sqrt(5/4))
p_theta <- -1 / sqrt(8 * 3.14159265) * exp(-theta ^ 2 /8)
sum(0.01 *p * p_theta)
# c
plot(theta, p, type = 'l')
theta <- -seq(-10, 10, 0.01)
p <- pnorm(theta/4 + 0.675 * sqrt(5/4)) - pnorm(theta / 4 - 0.675 * sqrt(5/4))
p_theta <- -1 / sqrt(8 * 3.14159265) * exp(-theta ^ 2 /8)
sum(0.01 *p * p_theta)
# c
plot(theta, p, type = 'l')
p_theta <- 1 / sqrt(8 * 3.14159265) * exp(-theta ^ 2 /8)
sum(0.01 *p * p_theta)
# c
plot(theta, p, type = 'l')
help(log)
y <- c(16,9,10,13,19,20,18,17,35,55)
n <- c(58,90,48,57,103,57,86,112,273,64) + y
r <- y/n
m <- mean(r)
v <- var(r)
alpha <- (m ^ 2 * (1 - m)) / v - m
beta <- (m * (1 - m) ^ 2) / v + m - 1
log(alpha/beta)
log(alpha+beta)
y <- c(16,9,10,13,19,20,18,17,35,55)
n <- c(58,90,48,57,103,57,86,112,273,64) + y
r <- y/n
alpha <- (mean(r) ^ 2 * (1 - mean(r))) / var(r) - mean(r)
beta <- (mean(r) * (1 - mean(r)) ^ 2) / var(r) + mean(r) - 1
y <- c(16,9,10,13,19,20,18,17,35,55)
n <- c(58,90,48,57,103,57,86,112,273,64) + y
r <- y/n
alpha <- (mean(r) ^ 2 * (1 - mean(r))) / var(r) - mean(r)
beta <- (mean(r) * (1 - mean(r)) ^ 2) / var(r) + mean(r) - 1
a <- seq(-2.0, -0.5, length.out=1000)
b <- seq(1.0, 4.5, length.out=1000)
prior <- matrix(0, 1000, 1000)
posterior <- matrix(0, 1000, 1000)
for (i in 1:1000){
for (j in 1:1000){
alpha_sim <- exp(a[i] + b[j]) / (exp(a[i]) + 1)
beta_sim <- exp(b[j]) / (exp(a[i]) + 1)
prior[i, j] <- (alpha_sim + beta_sim) ^ (-2.5) * alpha_sim * beta_sim
posterior[i, j] <- prior[i, j] * prod(beta(alpha_sim + y, beta_sim + n - y) / beta(alpha_sim, beta_sim))
}
}
log(alpha/beta)log(alpha+beta)
y <- c(16,9,10,13,19,20,18,17,35,55)
n <- c(58,90,48,57,103,57,86,112,273,64) + y
r <- y/n
alpha <- (mean(r) ^ 2 * (1 - mean(r))) / var(r) - mean(r)
beta <- (mean(r) * (1 - mean(r)) ^ 2) / var(r) + mean(r) - 1
log(alpha + beta)
log(alpha / beta)
a <- seq(-2.0, -0.5, length.out=1000)
b <- seq(1.0, 4.5, length.out=1000)
prior <- matrix(0, 1000, 1000)
posterior <- matrix(0, 1000, 1000)
for (i in 1:1000){
for (j in 1:1000){
alpha_sim <- exp(a[i] + b[j]) / (exp(a[i]) + 1)
beta_sim <- exp(b[j]) / (exp(a[i]) + 1)
prior[i, j] <- (alpha_sim + beta_sim) ^ (-2.5) * alpha_sim * beta_sim
posterior[i, j] <- prior[i, j] * prod(beta(alpha_sim + y, beta_sim + n - y) / beta(alpha_sim, beta_sim))
}
}
contour(a, b, posterior/max(posterior), xlab="log(alpha/beta)", ylab="log(alpha+beta)", main="marginal posterior density")
plot(alpha_sim, beta_sim, main="simulations from the joint posterior distribution")
post_alpha <- apply(posterior, 1, sum)
alpha_sim = rep(0,1000)
beta_sim = rep(0,1000)
for(i in 1:1000){
loc <- sample(1000, 1, prob=post_alpha)
a_sim <- a[loc]
post_beta <- posterior[loc,]
b_sim <- sample(b, 1, prob=post_beta)
alpha_sim[i] <- exp(a_sim + b_sim) / (exp(a_sim) + 1)
beta_sim[i] <- exp(b_sim) / (exp(a_sim) + 1)
}
plot(alpha_sim, beta_sim, main="simulations from the joint posterior distribution")
install.packages("plotrix")
library(plotrix)
theta <- matrix(rep(0, 10 * 1000), 10, 1000)
for(i in 1:10)
for(j in 1:1000){
{
theta[i,j]=rbeta(1, alpha_sim[j] + y[i], beta_sim[j] + n[i] - y[i])
}
}
mean <- rowMeans(theta)
median <- apply(theta, 1, median)
lower <- vector()
upper <- vector()
for(i in 1:10){
lower[i] <- quantile(theta[i,],0.025)
upper[i] <- quantile(theta[i,],0.975)
}
theta_raw <- y / n
plotCI(theta_raw, median, ui=upper, li=lower, xlab="observed rate", ylab="95% condidence interval of simulation")
abline(a=0,b=1)
mean(upper)
mean(lower)
mean(lower)
mean(upper)
y <- c(16,9,10,13,19,20,18,17,35,55)
n <- c(58,90,48,57,103,57,86,112,273,64) + y
r <- y/n
alpha <- (mean(r) ^ 2 * (1 - mean(r))) / var(r) - mean(r)
beta <- (mean(r) * (1 - mean(r)) ^ 2) / var(r) + mean(r) - 1
a <- seq(-2.0, -0.5, length.out=1000)
b <- seq(1.0, 4.5, length.out=1000)
prior <- matrix(0, 1000, 1000)
posterior <- matrix(0, 1000, 1000)
for (i in 1:1000){
for (j in 1:1000){
alpha_sim <- exp(a[i] + b[j]) / (exp(a[i]) + 1)
beta_sim <- exp(b[j]) / (exp(a[i]) + 1)
prior[i, j] <- (alpha_sim + beta_sim) ^ (-2.5) * alpha_sim * beta_sim
posterior[i, j] <- prior[i, j] * prod(beta(alpha_sim + y, beta_sim + n - y) / beta(alpha_sim, beta_sim))
}
}
contour(a, b, posterior/max(posterior), xlab="log(alpha/beta)", ylab="log(alpha+beta)", main="marginal posterior density")
post_alpha <- apply(posterior, 1, sum)
alpha_sim = rep(0,1000)
beta_sim = rep(0,1000)
for(i in 1:1000){
loc <- sample(1000, 1, prob=post_alpha)
a_sim <- a[loc]
post_beta <- posterior[loc,]
b_sim <- sample(b, 1, prob=post_beta)
alpha_sim[i] <- exp(a_sim + b_sim) / (exp(a_sim) + 1)
beta_sim[i] <- exp(b_sim) / (exp(a_sim) + 1)
}
plot(alpha_sim, beta_sim, main="simulations from the joint posterior distribution")
# c
library(plotrix)
theta <- matrix(rep(0, 10 * 1000), 10, 1000)
for(i in 1:10)
for(j in 1:1000){
{
theta[i,j]=rbeta(1, alpha_sim[j] + y[i], beta_sim[j] + n[i] - y[i])
}
}
mean <- rowMeans(theta)
median <- apply(theta, 1, median)
lower <- vector()
upper <- vector()
for(i in 1:10){
lower[i] <- quantile(theta[i,],0.025)
upper[i] <- quantile(theta[i,],0.975)
}
theta_raw <- y / n
plotCI(theta_raw, median, ui=upper, li=lower, xlab="observed rate", ylab="95% condidence interval of simulation")
abline(a=0,b=1)
# d
mean(lower)
mean(upper)
# e
y_new<-seq(0, 100, 0.1)
post_pred<-rep(0, 101)
for(i in 1:1000){
post_pred <- post_pred + dbinom(y_new, 100, theta[7,i])
}
post <- cumsum(post_pred) / sum(post_pred)
low_new<-max(which(post<0.025))
upp_new<-min(which(post<0.025))
low_new
upp_new
y_new<-seq(0, 100, 0.1)
theta_avg<-colMeans(theta)
post_pred<-rep(0, 101)
library(plotrix)
theta <- matrix(rep(0, 10 * 1000), 10, 1000)
for(i in 1:10)
for(j in 1:1000){
{
theta[i,j]=rbeta(1, alpha_sim[j] + y[i], beta_sim[j] + n[i] - y[i])
}
}
mean <- rowMeans(theta)
median <- apply(theta, 1, median)
lower <- vector()
upper <- vector()
for(i in 1:10){
lower[i] <- quantile(theta[i,],0.025)
upper[i] <- quantile(theta[i,],0.975)
}
theta_raw <- y / n
plotCI(theta_raw, median, ui=upper, li=lower, xlab="observed rate", ylab="95% condidence interval of simulation")
abline(a=0,b=1)
# d
mean(lower)
mean(upper)
y <- c(16,9,10,13,19,20,18,17,35,55)
n <- c(58,90,48,57,103,57,86,112,273,64) + y
r <- y/n
alpha <- (mean(r) ^ 2 * (1 - mean(r))) / var(r) - mean(r)
beta <- (mean(r) * (1 - mean(r)) ^ 2) / var(r) + mean(r) - 1
a <- seq(-2.0, -0.5, length.out=1000)
b <- seq(1.0, 4.5, length.out=1000)
prior <- matrix(0, 1000, 1000)
posterior <- matrix(0, 1000, 1000)
for (i in 1:1000){
for (j in 1:1000){
alpha_sim <- exp(a[i] + b[j]) / (exp(a[i]) + 1)
beta_sim <- exp(b[j]) / (exp(a[i]) + 1)
prior[i, j] <- (alpha_sim + beta_sim) ^ (-2.5) * alpha_sim * beta_sim
posterior[i, j] <- prior[i, j] * prod(beta(alpha_sim + y, beta_sim + n - y) / beta(alpha_sim, beta_sim))
}
}
contour(a, b, posterior/max(posterior), xlab="log(alpha/beta)", ylab="log(alpha+beta)", main="marginal posterior density")
post_alpha <- apply(posterior, 1, sum)
alpha_sim = rep(0,1000)
beta_sim = rep(0,1000)
for(i in 1:1000){
loc <- sample(1000, 1, prob=post_alpha)
a_sim <- a[loc]
post_beta <- posterior[loc,]
b_sim <- sample(b, 1, prob=post_beta)
alpha_sim[i] <- exp(a_sim + b_sim) / (exp(a_sim) + 1)
beta_sim[i] <- exp(b_sim) / (exp(a_sim) + 1)
}
plot(alpha_sim, beta_sim, main="simulations from the joint posterior distribution")
# c
library(plotrix)
theta <- matrix(rep(0, 10 * 1000), 10, 1000)
for(i in 1:10)
for(j in 1:1000){
{
theta[i,j]=rbeta(1, alpha_sim[j] + y[i], beta_sim[j] + n[i] - y[i])
}
}
mean <- rowMeans(theta)
median <- apply(theta, 1, median)
lower <- vector()
upper <- vector()
for(i in 1:10){
lower[i] <- quantile(theta[i,],0.025)
upper[i] <- quantile(theta[i,],0.975)
}
theta_raw <- y / n
plotCI(theta_raw, median, ui=upper, li=lower, xlab="observed rate", ylab="95% condidence interval of simulation")
abline(a=0,b=1)
# d
mean(lower)
mean(upper)
# e
y_new<-seq(0, 100, 0.1)
theta_avg<-colMeans(theta)
post_pred<-rep(0, 101)
for (i in 1:1000){
post.pred<-post.pred+dbinom(y_new, 100, theta_avg[i]) / 1000
}
for (i in 1:1000){    post.pred<-post_pred + dbinom(y_new, 100, theta_avg[i]) / 1000}
for (i in 1:1000){
post.pred<-post_pred + dbinom(y_new, 100, theta_avg[i]) / 1000
}
prob <- 0
CI <- c(0, 0)
for (i in y_new){
prob <- prob + post_pred[i + 1]
if(prob <= 0.025) CI[1] <- i
if(prob >= 0.975) {
CI[2] <- i
break
}
}
CI
